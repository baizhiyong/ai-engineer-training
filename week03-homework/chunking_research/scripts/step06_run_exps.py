#!/usr/bin/env python3
"""
åˆ†å—å®žéªŒè¿è¡Œè„šæœ¬

æ”¯æŒä¸‰ç§æ‰§è¡Œæ¨¡å¼ï¼š
1. sequential - é¡ºåºæ‰§è¡Œï¼ˆé»˜è®¤ï¼Œç¨³å®šå¯é ï¼‰
2. parallel - å¹¶è¡Œæ‰§è¡Œï¼ˆé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½é‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼‰
3. auto - è‡ªåŠ¨æ¨¡å¼ï¼ˆå…ˆæµ‹è¯•å†é€‰æ‹©æœ€ä½³æ–¹å¼ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
python run_exps.py                           # é»˜è®¤é¡ºåºæ¨¡å¼ï¼ˆæŽ¨èï¼‰
python run_exps.py --mode sequential         # æ˜¾å¼æŒ‡å®šé¡ºåºæ¨¡å¼
python run_exps.py --mode parallel --jobs 4  # å¹¶è¡Œæ¨¡å¼ï¼Œ4ä¸ªçº¿ç¨‹
python run_exps.py --mode auto --jobs 2      # è‡ªåŠ¨æ¨¡å¼ï¼Œæœ€å¤š2ä¸ªçº¿ç¨‹
"""
import os, json, yaml, itertools
from pathlib import Path
from tqdm import tqdm
from joblib import Parallel, delayed
from typing import Tuple

# æ·»åŠ NLTKæ•°æ®åˆå§‹åŒ–
try:
    import nltk
    # å°è¯•ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("ä¸‹è½½NLTKæ•°æ®...")
        nltk.download('punkt', quiet=True)
        
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)
        
except ImportError:
    print("NLTKæœªå®‰è£…ï¼Œè·³è¿‡NLTKæ•°æ®åˆå§‹åŒ–")

from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels
from llama_index.core.node_parser import (
    SentenceSplitter,
    TokenTextSplitter,
    SentenceWindowNodeParser,
    MarkdownNodeParser,
)
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# ---------- 1. å…¨å±€é…ç½® ----------
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    is_chat_model=True,
    temperature=0.3,
)
Settings.embed_model = DashScopeEmbedding(
    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,
    embed_batch_size=6,
    embed_input_length=8192
)

def _get_project_dirs() -> Tuple[str, str]:
    """èŽ·å–é¡¹ç›®ç›®å½•è·¯å¾„
    
    åŠ¨æ€è®¡ç®—ç›¸å¯¹äºŽå½“å‰è„šæœ¬çš„ç›®å½•è·¯å¾„ï¼Œ
    ç¡®ä¿æ— è®ºåœ¨å“ªé‡Œè¿è¡Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°
    
    Returns:
        tuple: (data_clean_dir, outputs_dir)
    """
    # èŽ·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ä»Žscriptsç›®å½•å‘ä¸Šæ‰¾åˆ°chunking_researchç›®å½•
    # scripts -> chunking_research -> data -> clean
    chunking_research_dir = os.path.dirname(current_script_dir)
    data_clean_dir = os.path.join(chunking_research_dir, "data", "clean")
    outputs_dir = os.path.join(chunking_research_dir, "outputs")
    
    return data_clean_dir, outputs_dir

data_dir_str, output_dir_str = _get_project_dirs()
DATA_DIR: Path = Path(data_dir_str)
OUTPUT_DIR: Path = Path(output_dir_str)
OUTPUT_DIR.mkdir(exist_ok=True)

# ---------- 2. è½½å…¥å®žéªŒé…ç½® ----------
def _get_config_path():
    """èŽ·å–å®žéªŒé…ç½®æ–‡ä»¶è·¯å¾„"""
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    chunking_research_dir = os.path.dirname(current_script_dir)
    return os.path.join(chunking_research_dir, "experiments.yaml")

cfg = yaml.safe_load(open(_get_config_path()))

# æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
if not DATA_DIR.exists():
    print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
    print("è¯·ç¡®ä¿ data/clean ç›®å½•å­˜åœ¨å¹¶åŒ…å«æ–‡æ¡£æ–‡ä»¶")
    exit(1)

print(f"ä»Ž {DATA_DIR} åŠ è½½æ–‡æ¡£...")
documents = SimpleDirectoryReader(DATA_DIR).load_data()
print(f"å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

# ä»Ž qa.json æ–‡ä»¶ä¸­æå–é—®é¢˜
def _get_qa_file_path():
    """èŽ·å– qa.json æ–‡ä»¶è·¯å¾„"""
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    chunking_research_dir = os.path.dirname(current_script_dir)
    return os.path.join(chunking_research_dir, "outputs", "qa.json")

def _load_questions_from_qa_json():
    """ä»Ž qa.json æ–‡ä»¶ä¸­åŠ è½½é—®é¢˜åˆ—è¡¨"""
    qa_file_path = _get_qa_file_path()
    try:
        with open(qa_file_path, 'r', encoding='utf-8') as f:
            qa_data = json.load(f)
        # æå–æ‰€æœ‰é—®é¢˜ï¼ŒåŽ»é‡
        questions = list(set(item['question'] for item in qa_data if 'question' in item))
        return questions
    except FileNotFoundError:
        print(f"è­¦å‘Šï¼šqa.json æ–‡ä»¶ä¸å­˜åœ¨ ({qa_file_path})ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
        return [
            "è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ–‡ä¸­æåˆ°äº†å“ªäº›å…³é”®æŠ€æœ¯ï¼Ÿ",
            "ä½œè€…ç»™å‡ºäº†å“ªäº›æ•°æ®æˆ–æ¡ˆä¾‹ï¼Ÿ",
            "è¿™äº›æŠ€æœ¯å¯èƒ½å¸¦æ¥å“ªäº›å½±å“ï¼Ÿ",
            "æ–‡ç« çš„ç»“æž„æ˜¯æ€Žæ ·çš„ï¼Ÿ"
        ]
    except Exception as e:
        print(f"è­¦å‘Šï¼šåŠ è½½ qa.json æ–‡ä»¶å¤±è´¥ ({e})ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
        return [
            "è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ–‡ä¸­æåˆ°äº†å“ªäº›å…³é”®æŠ€æœ¯ï¼Ÿ",
            "ä½œè€…ç»™å‡ºäº†å“ªäº›æ•°æ®æˆ–æ¡ˆä¾‹ï¼Ÿ",
            "è¿™äº›æŠ€æœ¯å¯èƒ½å¸¦æ¥å“ªäº›å½±å“ï¼Ÿ",
            "æ–‡ç« çš„ç»“æž„æ˜¯æ€Žæ ·çš„ï¼Ÿ"
        ]

QUESTIONS = _load_questions_from_qa_json()

# ---------- 3. splitter å·¥åŽ‚ ----------
def get_splitter(name: str, params: dict):
    if name == "SentenceSplitter":
        return SentenceSplitter(**params)
    if name == "TokenTextSplitter":
        return TokenTextSplitter(**params)
    if name == "SentenceWindowNodeParser":
        # window_size æ˜¯ç±»æ–¹æ³•å‚æ•°
        return SentenceWindowNodeParser.from_defaults(
            window_size=params["window_size"],
            window_metadata_key="window",
            original_text_metadata_key="original_text",
        )
    if name == "MarkdownNodeParser":
        return MarkdownNodeParser()
    raise ValueError(f"unknown splitter {name}")

# ---------- 4. å•ç»„å®žéªŒ ----------
def run_one(exp_id: str, splitter, docs, qs):
    try:
        print(f"å¼€å§‹å®žéªŒ: {exp_id}")
        nodes = splitter.get_nodes_from_documents(docs)
        print(f"ç”Ÿæˆäº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
        index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)
        print(f"åˆ›å»ºäº†å‘é‡ç´¢å¼•")

        # SentenceWindow éœ€è¦ç‰¹æ®ŠåŽå¤„ç†
        post = None
        if isinstance(splitter, SentenceWindowNodeParser):
            post = [MetadataReplacementPostProcessor(target_metadata_key="window")]

        engine = index.as_query_engine(similarity_top_k=5, node_postprocessors=post or [])

        results = []
        for i, q in enumerate(qs):
            print(f"å¤„ç†é—®é¢˜ {i+1}/{len(qs)}: {q[:50]}...")
            resp = engine.query(q)
            ctx = [n.node.get_content() for n in resp.source_nodes]
            results.append({"question": q, "answer": str(resp), "contexts": ctx})
            
        out_file = OUTPUT_DIR / f"{exp_id}.json"
        out_file.write_text(json.dumps(results, ensure_ascii=False, indent=2))
        print(f"å®Œæˆå®žéªŒ: {exp_id}, ç»“æžœä¿å­˜åˆ°: {out_file}")
        return exp_id
        
    except Exception as e:
        print(f"å®žéªŒ {exp_id} å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

# ---------- 5. æ‰§è¡Œæ–¹å¼å°è£… ----------
def run_experiments_parallel(tasks, documents, questions, n_jobs=4):
    """å¹¶è¡Œæ‰§è¡Œå®žéªŒ
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (exp_id, splitter)
        documents: æ–‡æ¡£åˆ—è¡¨
        questions: é—®é¢˜åˆ—è¡¨
        n_jobs: å¹¶è¡Œä»»åŠ¡æ•°
        
    Returns:
        list: æˆåŠŸå®Œæˆçš„å®žéªŒIDåˆ—è¡¨
    """
    # è¿‡æ»¤æŽ‰å·²å­˜åœ¨ç»“æžœæ–‡ä»¶çš„ä»»åŠ¡
    filtered_tasks = []
    skipped_count = 0
    
    for exp_id, sp in tasks:
        out_file = OUTPUT_DIR / f"{exp_id}.json"
        if out_file.exists():
            print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„å®žéªŒ: {exp_id}")
            skipped_count += 1
        else:
            filtered_tasks.append((exp_id, sp))
    
    if skipped_count > 0:
        print(f"è·³è¿‡äº† {skipped_count} ä¸ªå·²å®Œæˆçš„å®žéªŒ")
    
    if not filtered_tasks:
        print("æ‰€æœ‰å®žéªŒéƒ½å·²å®Œæˆï¼")
        return []
    
    print(f"ä½¿ç”¨å¹¶è¡Œæ¨¡å¼æ‰§è¡Œ {len(filtered_tasks)} ä¸ªä»»åŠ¡ï¼Œå¹¶è¡Œåº¦: {min(n_jobs, len(filtered_tasks))}")
    
    results = Parallel(n_jobs=min(n_jobs, len(filtered_tasks)), backend="threading")(
        delayed(run_one)(exp_id, sp, documents, questions) for exp_id, sp in tqdm(filtered_tasks, desc="å¹¶è¡Œæ‰§è¡Œ")
    )
    return list(results)

def run_experiments_sequential(tasks, documents, questions):
    """é¡ºåºæ‰§è¡Œå®žéªŒ
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (exp_id, splitter)
        documents: æ–‡æ¡£åˆ—è¡¨
        questions: é—®é¢˜åˆ—è¡¨
        
    Returns:
        list: æˆåŠŸå®Œæˆçš„å®žéªŒIDåˆ—è¡¨
    """
    # ç»Ÿè®¡å’Œè¿‡æ»¤ä»»åŠ¡
    total_tasks = len(tasks)
    skipped_count = 0
    completed_count = 0
    failed_count = 0
    
    print(f"ä½¿ç”¨é¡ºåºæ¨¡å¼æ‰§è¡Œ {total_tasks} ä¸ªä»»åŠ¡")
    
    results = []
    for exp_id, sp in tqdm(tasks, desc="é¡ºåºæ‰§è¡Œ"):
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        out_file = OUTPUT_DIR / f"{exp_id}.json"
        if out_file.exists():
            print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„å®žéªŒ: {exp_id}")
            skipped_count += 1
            results.append(exp_id)  # å°†å·²å­˜åœ¨çš„å®žéªŒä¹Ÿè®¡å…¥ç»“æžœ
            continue
            
        try:
            result = run_one(exp_id, sp, documents, questions)
            results.append(result)
            completed_count += 1
            print(f"âœ“ å®Œæˆ: {exp_id}")
        except Exception as task_error:
            print(f"âœ— ä»»åŠ¡ {exp_id} å¤±è´¥: {task_error}")
            failed_count += 1
            continue
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nðŸ“Š æ‰§è¡Œç»Ÿè®¡:")
    print(f"   æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"   è·³è¿‡å·²å®Œæˆ: {skipped_count}")
    print(f"   æ–°å®Œæˆ: {completed_count}")
    print(f"   å¤±è´¥: {failed_count}")
    print(f"   æˆåŠŸçŽ‡: {len(results)}/{total_tasks} ({len(results)/total_tasks*100:.1f}%)")
    
    return results

def run_experiments_auto(tasks, documents, questions, prefer_parallel=True, n_jobs=4):
    """è‡ªåŠ¨é€‰æ‹©æ‰§è¡Œæ–¹å¼
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨
        documents: æ–‡æ¡£åˆ—è¡¨
        questions: é—®é¢˜åˆ—è¡¨
        prefer_parallel: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨å¹¶è¡Œæ¨¡å¼
        n_jobs: å¹¶è¡Œä»»åŠ¡æ•°
        
    Returns:
        list: æˆåŠŸå®Œæˆçš„å®žéªŒIDåˆ—è¡¨
    """
    print(f"å…± {len(tasks)} ç»„å®žéªŒï¼Œå¼€å§‹è¿è¡Œ...")
    
    # å…ˆæ£€æŸ¥å·²å®Œæˆçš„ä»»åŠ¡
    remaining_tasks = []
    completed_existing = []
    
    for exp_id, sp in tasks:
        out_file = OUTPUT_DIR / f"{exp_id}.json"
        if out_file.exists():
            print(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„å®žéªŒ: {exp_id}")
            completed_existing.append(exp_id)
        else:
            remaining_tasks.append((exp_id, sp))
    
    if completed_existing:
        print(f"å‘çŽ° {len(completed_existing)} ä¸ªå·²å®Œæˆçš„å®žéªŒ")
    
    if not remaining_tasks:
        print("æ‰€æœ‰å®žéªŒéƒ½å·²å®Œæˆï¼")
        return completed_existing
    
    # å…ˆè¿è¡Œä¸€ä¸ªæµ‹è¯•ä»»åŠ¡
    print(f"æµ‹è¯•è¿è¡Œç¬¬ä¸€ä¸ªä»»åŠ¡: {remaining_tasks[0][0]}")
    try:
        test_result = run_one(remaining_tasks[0][0], remaining_tasks[0][1], documents, questions)
        print(f"âœ“ æµ‹è¯•æˆåŠŸ: {test_result}")
        # ç§»é™¤å·²æµ‹è¯•çš„ä»»åŠ¡
        final_remaining_tasks = remaining_tasks[1:]
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨é¡ºåºæ¨¡å¼æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡")
        return completed_existing + run_experiments_sequential(remaining_tasks, documents, questions)
    
    if not final_remaining_tasks:
        print("æµ‹è¯•ä»»åŠ¡æ˜¯æœ€åŽä¸€ä¸ªä»»åŠ¡ï¼Œå·²å…¨éƒ¨å®Œæˆ")
        return completed_existing + [test_result]
    
    # æ ¹æ®åå¥½é€‰æ‹©æ‰§è¡Œæ–¹å¼
    if prefer_parallel:
        try:
            print("å°è¯•å¹¶è¡Œæ‰§è¡Œå‰©ä½™ä»»åŠ¡...")
            parallel_results = run_experiments_parallel(final_remaining_tasks, documents, questions, n_jobs)
            results = completed_existing + [test_result] + parallel_results
            print(f"âœ“ å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œæ€»å…±å®Œæˆ {len(results)} ä¸ªä»»åŠ¡")
            return results
        except Exception as e:
            print(f"âœ— å¹¶è¡Œæ‰§è¡Œå¤±è´¥: {e}")
            print("å›žé€€åˆ°é¡ºåºæ‰§è¡Œ...")
            sequential_results = run_experiments_sequential(final_remaining_tasks, documents, questions)
            results = completed_existing + [test_result] + sequential_results
            print(f"âœ“ é¡ºåºæ‰§è¡Œå®Œæˆï¼Œæ€»å…±å®Œæˆ {len(results)} ä¸ªä»»åŠ¡")
            return results
    else:
        print("ä½¿ç”¨é¡ºåºæ‰§è¡Œ...")
        sequential_results = run_experiments_sequential(final_remaining_tasks, documents, questions)
        results = completed_existing + [test_result] + sequential_results
        print(f"âœ“ é¡ºåºæ‰§è¡Œå®Œæˆï¼Œæ€»å…±å®Œæˆ {len(results)} ä¸ªä»»åŠ¡")
        return results

# ---------- 6. æž„é€ å‚æ•°ç¬›å¡å°”ç§¯ ----------
tasks = []
for item in cfg["splitters"]:
    cls_name = item["cls"]
    # params æ˜¯åˆ—è¡¨ï¼Œéœ€è¦éåŽ†æ¯ä¸ªå‚æ•°ç»„åˆ
    for param_group in item["params"]:
        if not param_group:  # ç©ºå­—å…¸çš„æƒ…å†µ
            param_dict = {}
            splitter = get_splitter(cls_name, param_dict)
            exp_id = f"{item['name']}"
            tasks.append((exp_id, splitter))
        else:
            # æŠŠ dict-of-list å±•å¼€æˆ list-of-dict
            keys, values = zip(*param_group.items())
            for v in itertools.product(*values):
                param_dict = dict(zip(keys, v))
                splitter = get_splitter(cls_name, param_dict)
                # å®žéªŒç¼–å·ï¼šSplitter_cls_param1_val1_param2_val2
                exp_id = f"{item['name']}_" + "_".join(f"{k}_{v}" for k, v in param_dict.items())
                tasks.append((exp_id, splitter))

# ---------- 7. æ‰§è¡Œå®žéªŒ ----------
if __name__ == "__main__":
    import sys
    
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ‰§è¡Œæ¨¡å¼
    # python run_exps.py                        # é»˜è®¤é¡ºåºæ¨¡å¼
    # python run_exps.py --mode parallel --jobs 4
    # python run_exps.py --mode sequential
    # python run_exps.py --mode auto
    
    mode = "sequential"  # é»˜è®¤é¡ºåºæ¨¡å¼ï¼Œæ›´ç¨³å®šå¯é 
    n_jobs = 4          # é»˜è®¤å¹¶è¡Œæ•°ï¼ˆä»…åœ¨å¹¶è¡Œ/è‡ªåŠ¨æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
    
    # ç®€å•çš„å‘½ä»¤è¡Œå‚æ•°è§£æž
    if len(sys.argv) > 1:
        for i, arg in enumerate(sys.argv[1:], 1):
            if arg == "--mode" and i + 1 < len(sys.argv):
                mode = sys.argv[i + 1]
            elif arg == "--jobs" and i + 1 < len(sys.argv):
                try:
                    n_jobs = int(sys.argv[i + 1])
                except ValueError:
                    print(f"è­¦å‘Š: æ— æ•ˆçš„å¹¶è¡Œæ•° '{sys.argv[i + 1]}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ {n_jobs}")
    
    print(f"æ‰§è¡Œæ¨¡å¼: {mode}" + (f", å¹¶è¡Œæ•°: {n_jobs}" if mode in ["parallel", "auto"] else ""))
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    if mode == "parallel":
        results = run_experiments_parallel(tasks, documents, QUESTIONS, n_jobs)
        print(f"\nðŸŽ‰ å¹¶è¡Œæ‰§è¡Œå®Œæˆï¼Œå…±å®Œæˆ {len(results)} ä¸ªä»»åŠ¡ï¼Œç»“æžœè§ outputs/*.json")
    elif mode == "sequential":
        results = run_experiments_sequential(tasks, documents, QUESTIONS)
        print(f"\nðŸŽ‰ é¡ºåºæ‰§è¡Œå®Œæˆï¼Œç»“æžœè§ outputs/*.json")
    else:  # auto mode
        results = run_experiments_auto(tasks, documents, QUESTIONS, prefer_parallel=True, n_jobs=n_jobs)
        print(f"\nðŸŽ‰ è‡ªåŠ¨æ‰§è¡Œå®Œæˆï¼Œç»“æžœè§ outputs/*.json")