#!/usr/bin/env python3
"""
RAGç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡è®¡ç®—å·¥å…·

åŠŸèƒ½è¯´æ˜ï¼š
========
æœ¬å·¥å…·ç”¨äºè®¡ç®—RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿçš„å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ£€ç´¢æ•ˆæœå’Œç”Ÿæˆè´¨é‡ä¸¤å¤§ç±»æŒ‡æ ‡ã€‚

è¯„ä¼°æŒ‡æ ‡è¯¦è§£ï¼š
============

æ£€ç´¢æ•ˆæœæŒ‡æ ‡ï¼š
-----------
1. Hit@1ï¼š
   - å«ä¹‰ï¼šæ£€ç´¢çš„Top-1æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å«ç­”æ¡ˆ
   - è®¡ç®—æ–¹æ³•ï¼šæ£€ç´¢åˆ°çš„ç¬¬1ä¸ªæ–‡æ¡£ä¸æ ‡å‡†ç­”æ¡ˆçš„Rouge-L >= 0.5å³ç®—å‘½ä¸­
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
   - åæ˜ ï¼šæ£€ç´¢ç³»ç»Ÿçš„ç²¾ç¡®åº¦

2. Hit@3ï¼š
   - å«ä¹‰ï¼šæ£€ç´¢çš„Top-3æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å«ç­”æ¡ˆ
   - è®¡ç®—æ–¹æ³•ï¼šæ£€ç´¢åˆ°çš„å‰3ä¸ªæ–‡æ¡£ä¸­ä»»ä¸€ä¸ªä¸æ ‡å‡†ç­”æ¡ˆçš„Rouge-L >= 0.5å³ç®—å‘½ä¸­
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
   - åæ˜ ï¼šæ£€ç´¢ç³»ç»Ÿçš„å¬å›èƒ½åŠ›ï¼ˆç›¸å¯¹å®½æ¾ï¼‰

3. Hit@5ï¼š
   - å«ä¹‰ï¼šæ£€ç´¢çš„Top-5æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å«ç­”æ¡ˆ
   - è®¡ç®—æ–¹æ³•ï¼šæ£€ç´¢åˆ°çš„å‰5ä¸ªæ–‡æ¡£ä¸­ä»»ä¸€ä¸ªä¸æ ‡å‡†ç­”æ¡ˆçš„Rouge-L >= 0.5å³ç®—å‘½ä¸­
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
   - åæ˜ ï¼šæ£€ç´¢ç³»ç»Ÿçš„å¬å›èƒ½åŠ›ï¼ˆæ›´å®½æ¾ï¼‰

4. MRR (Mean Reciprocal Rank)ï¼š
   - å«ä¹‰ï¼šå¹³å‡å€’æ•°æ’åï¼Œè¡¡é‡ç›¸å…³æ–‡æ¡£åœ¨æ£€ç´¢ç»“æœä¸­çš„æ’åºè´¨é‡
   - è®¡ç®—æ–¹æ³•ï¼š1/é¦–æ¬¡å‘½ä¸­ä½ç½®çš„å¹³å‡å€¼ï¼Œæœªå‘½ä¸­ä¸º0
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
   - åæ˜ ï¼šæ£€ç´¢ç»“æœçš„æ’åºè´¨é‡ï¼Œè¶Šæ—©å‘½ä¸­åˆ†æ•°è¶Šé«˜

5. Redundancyï¼š
   - å«ä¹‰ï¼šæ£€ç´¢æ–‡æ¡£é—´çš„å†—ä½™åº¦
   - è®¡ç®—æ–¹æ³•ï¼š1 - (å»é‡tokenæ•° / æ€»tokenæ•°)
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šä½è¶Šå¥½
   - åæ˜ ï¼šæ£€ç´¢ç»“æœçš„å¤šæ ·æ€§ï¼Œå†—ä½™åº¦ä½è¯´æ˜æ–‡æ¡£å·®å¼‚åŒ–ç¨‹åº¦é«˜

ç”Ÿæˆè´¨é‡æŒ‡æ ‡ï¼š
-----------
6. BLEUï¼š
   - å«ä¹‰ï¼šåŸºäºn-gramé‡å çš„ç”Ÿæˆæ–‡æœ¬è´¨é‡è¯„ä¼°
   - è®¡ç®—æ–¹æ³•ï¼šç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„n-gramç²¾ç¡®åº¦
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
   - åæ˜ ï¼šç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒç­”æ¡ˆçš„è¯æ±‡å±‚é¢ç›¸ä¼¼åº¦

7. BERTScore_F1ï¼š
   - å«ä¹‰ï¼šåŸºäºBERTåµŒå…¥çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°
   - è®¡ç®—æ–¹æ³•ï¼šç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆåœ¨BERTè¯­ä¹‰ç©ºé—´çš„F1åˆ†æ•°
   - å–å€¼èŒƒå›´ï¼š[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
   - åæ˜ ï¼šç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰å±‚é¢ç›¸ä¼¼åº¦ï¼Œæ¯”BLEUæ›´èƒ½æ•æ‰è¯­ä¹‰ä¿¡æ¯

è¾“å…¥è¾“å‡ºï¼š
========
è¾“å…¥ï¼š
- outputsç›®å½•ä¸‹çš„å®éªŒç»“æœJSONæ–‡ä»¶ï¼ˆrun_exps.pyç”Ÿæˆï¼‰
- qa.jsonæ–‡ä»¶ï¼ˆauto_qa.pyç”Ÿæˆçš„æ ‡å‡†é—®ç­”å¯¹ï¼‰

è¾“å‡ºï¼š
- metrics.csvï¼šåŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„è¯„ä¼°ç»“æœè¡¨æ ¼
- bertscore_bar.pngï¼šBERTScore_F1æŒ‡æ ‡çš„å¯è§†åŒ–æŸ±çŠ¶å›¾

ä½¿ç”¨æ–¹å¼ï¼š
========
ç›´æ¥è¿è¡Œè„šæœ¬å³å¯ï¼š
python scoring.py

æ³¨æ„äº‹é¡¹ï¼š
========
- ç¡®ä¿å·²å®‰è£…ä¾èµ–åŒ…ï¼špip install evaluate bert-score rouge-chinese pandas seaborn tqdm
- è„šæœ¬ä¼šè‡ªåŠ¨å®šä½outputsç›®å½•ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®šè·¯å¾„
- HitæŒ‡æ ‡ä½¿ç”¨Rouge-L >= 0.5ä½œä¸ºå‘½ä¸­é˜ˆå€¼ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-10-29
ç‰ˆæœ¬: 1.2.0 (æ·»åŠ æŒ‡æ ‡è¯¦ç»†è¯´æ˜)
"""

#pip install evaluate bert-score rouge-chinese pandas seaborn tqdm rouge_score
import json, pandas as pd, numpy as np, os
from pathlib import Path
from tqdm import tqdm
import evaluate
from bert_score import score as bert_score

def _get_default_outputs_dir():
    """è·å–é»˜è®¤çš„outputsç›®å½•è·¯å¾„
    
    åŠ¨æ€è®¡ç®—ç›¸å¯¹äºå½“å‰è„šæœ¬çš„outputsç›®å½•è·¯å¾„ï¼Œ
    ç¡®ä¿æ— è®ºåœ¨å“ªé‡Œè¿è¡Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°
    
    Returns:
        outputsç›®å½•çš„ç»å¯¹è·¯å¾„
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # ä»scriptsç›®å½•å‘ä¸Šæ‰¾åˆ°chunking_researchç›®å½•ï¼Œç„¶åå®šä½outputs
    # scripts -> chunking_research -> outputs
    chunking_research_dir = os.path.dirname(current_script_dir)
    outputs_dir = os.path.join(chunking_research_dir, "outputs")
    
    return outputs_dir

# ---------- 1. è·¯å¾„ ----------
OUTPUTS_BASE = _get_default_outputs_dir()
EXP_DIR  = Path(OUTPUTS_BASE)               # run_exps.py ç”Ÿæˆçš„ json
QA_FILE  = Path(OUTPUTS_BASE) / "qa.json"  # auto_qa.py ç”Ÿæˆçš„é—®ç­”å¯¹
CSV_OUT  = Path(OUTPUTS_BASE) / "metrics.csv"

# ---------- 2. è½½å…¥æ ‡å‡†é—®ç­” ----------
qa_map = {item["question"]: item["answer"] for item in json.loads(QA_FILE.read_text())}

# ---------- 3. åˆå§‹åŒ– metric å‡½æ•° ----------
bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")          # ç”¨äº Hit åˆ¤æ–­

def hit_at_k(contexts, answer, k):
    """åªè¦ä»»ä¸€ top-k ä¸Šä¸‹æ–‡ä¸ç­”æ¡ˆ Rouge-L >= 0.5 å³ç®—å‘½ä¸­"""
    if not contexts or k == 0:
        return 0
    refs = [answer] * k
    hyps = contexts[:k]
    scores = []
    for h, r in zip(hyps, refs):
        result = rouge.compute(predictions=[h], references=[r])
        if result and "rougeL" in result:
            scores.append(result["rougeL"])
        else:
            scores.append(0.0)
    return int(max(scores) >= 0.5)

def redundancy_ratio(contexts):
    """å»é‡ token æ•° / æ€» token æ•°"""
    from collections import Counter
    tokens_all = [tok for c in contexts for tok in c.split()]
    tokens_set = set(tokens_all)
    return 1 - len(tokens_set) / max(len(tokens_all), 1)

# ---------- 4. é€å®éªŒæ‰“åˆ† ----------
records = []
for exp_json in tqdm(list(EXP_DIR.glob("*.json")), desc="Scoring"):
    if exp_json.name == "qa.json":
        continue
    data = json.loads(exp_json.read_text())
    exp_name = exp_json.stem

    hits_1, hits_3, hits_5, mrr, redunds = [], [], [], [], []
    bleu_scores, bert_p, bert_r, bert_f1 = [], [], [], []

    for row in data:
        q, gen_ans, ctxs = row["question"], row["answer"], row["contexts"]
        ans = qa_map.get(q, "")
        if not ans:
            continue

        # ---- æ£€ç´¢æŒ‡æ ‡ ----
        hits_1.append(hit_at_k(ctxs, ans, 1))
        hits_3.append(hit_at_k(ctxs, ans, 3))
        hits_5.append(hit_at_k(ctxs, ans, 5))
        # MRR
        first_hit = next((i + 1 for i, c in enumerate(ctxs) if hit_at_k([c], ans, 1)), 0)
        mrr.append(1 / first_hit if first_hit else 0.)
        redunds.append(redundancy_ratio(ctxs))

        # ---- ç”ŸæˆæŒ‡æ ‡ ----
        bleu_result = bleu.compute(predictions=[gen_ans], references=[ans])
        bleu_score = bleu_result["bleu"] if bleu_result and "bleu" in bleu_result else 0.0
        bleu_scores.append(bleu_score)
        
        try:
            # BERTScore è¿”å›ä¸‰ä¸ªå¼ é‡çš„å…ƒç»„
            bert_scores = bert_score([gen_ans], [ans], lang="zh", verbose=False, rescale_with_baseline=True)
            P, R, F1 = bert_scores
            
            # å°†å¼ é‡è½¬æ¢ä¸ºPythonæ•°å€¼
            bert_p.append(float(P[0]))
            bert_r.append(float(R[0])) 
            bert_f1.append(float(F1[0]))
        except Exception as e:
            print(f"BERTScoreè®¡ç®—é”™è¯¯: {e}")
            bert_p.append(0.0)
            bert_r.append(0.0)
            bert_f1.append(0.0)

    records.append({
        "exp": exp_name,
        "Hit@1": np.mean(hits_1),
        "Hit@3": np.mean(hits_3),
        "Hit@5": np.mean(hits_5),
        "MRR": np.mean(mrr),
        "Redundancy": np.mean(redunds),
        "BLEU": np.mean(bleu_scores),
        "BERTScore_F1": np.mean(bert_f1),
    })

# ---------- 5. ä¿å­˜ ----------
df = pd.DataFrame(records)
df.to_csv(CSV_OUT, index=False, float_format="%.4f")
print(f"âœ… å·²å†™å…¥ {CSV_OUT}  å…± {len(df)} ç»„å®éªŒ")

# ---------- 6. å¿«é€Ÿå¯è§†åŒ– ----------
import seaborn as sns, matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
sns.barplot(data=df, x="exp", y="BERTScore_F1", color="steelblue")
plt.xticks(rotation=45, ha="right")
plt.ylabel("BERTScore-F1")
plt.tight_layout()
chart_path = Path(OUTPUTS_BASE) / "bertscore_bar.png"
plt.savefig(chart_path, dpi=300)
print(f"ğŸ“Š å¿«é€ŸæŸ±çŠ¶å›¾å·²ä¿å­˜ â†’ {chart_path}")