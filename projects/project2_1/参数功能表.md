|参数| 功能|
|---|---|
|r| LoRA 低秩矩阵的秩，控制新增可训练参数的维度大小。r 越小，参数量越少，计算开销越低；r 越大，模型容量越高，但可能增加过拟合风险。常用值为 8 或 16。|
|lora_alpha| LoRA 的缩放系数，用于控制低秩更新的幅度。通常与 r 搭配使用（如 lora_alpha = 2×r），影响参数更新的强度和稳定性。|
|target_modules| 指定需要应用 LoRA 的模型模块名称列表。例如 "q_proj", "v_proj" 等是 Transformer 中的注意力头或前馈网络层，仅这些模块会被注入 LoRA 适配器。|
|lora_dropout| 在 LoRA 层中使用的 dropout 比例，用于防止过拟合。设置为 0.1 表示在训练时随机将 10% 的神经元输出置零。|
|bias| 指定是否对 LoRA 中的偏置项进行训练，可选值为 "none"（不训练）、"all"（训练所有偏置）、"lora_only"（仅训练 LoRA 引入的偏置）。设为 "none" 可减少额外参数。|
|task_type| 指定当前任务类型，如 SEQ_CLS（序列分类）、CAUSAL_LM（因果语言建模）等，影响内部配置适配。|
|output_dir| 训练过程中模型保存和输出的目录路径。|
|learning_rate| 优化器的学习率，控制参数更新步长。较小值更稳定，较大值收敛快但可能震荡。|
|per_device_train_batch_size| 每个设备（如 GPU）上的训练批次大小，影响内存占用和梯度估计质量。|
|per_device_eval_batch_size| 每个设备上的评估批次大小，通常与训练 batch size 相同。|
|num_train_epochs| 整体训练轮数，表示整个训练集被遍历的次数。|
|weight_decay| 权重衰减系数，用于 L2 正则化，防止模型过拟合。|
|evaluation_strategy| 评估策略，设为 "epoch" 表示每个 epoch 结束后进行一次验证评估。|
|save_strategy| 模型保存策略，"epoch" 表示每个 epoch 后保存一次检查点。|
|logging_dir| 日志文件保存路径，用于记录训练过程中的指标变化。|
|logging_steps| 每隔多少步记录一次日志信息（如 loss、学习率等）。|
|fp16| 是否启用半精度浮点（16位）训练，可显著降低显存消耗并加速训练。|
|gradient_checkpointing| 是否使用梯度检查点技术，在空间和时间之间权衡：节省显存但增加计算时间。|
|report_to| 指定是否将训练日志上报至第三方平台（如 WandB、TensorBoard），"none" 表示不上传。|
|load_best_model_at_end| 训练结束时是否自动加载验证集上表现最好的模型。|
|metric_for_best_model| 判断最佳模型所依据的评估指标，如 "eval_loss" 表示选择验证损失最小的模型。|
|greater_is_better| 配合 metric_for_best_model 使用，False 表示指标越小越好（如 loss），True 表示越大越好（如 accuracy）。|
|save_total_limit| 最多保留的检查点数量，超出后旧的检查点将被删除，有助于节省磁盘空间。|